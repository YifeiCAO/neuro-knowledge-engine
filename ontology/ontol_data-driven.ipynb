{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will take a data-driven approach to generating word lists for mental functions that are related to brain circuitry. The overall process is as follows:\n",
    "\n",
    "1. Cluster brain structures into circuits by PMI-weighted co-occurrences with mental function terms.\n",
    "2. Identify the mental function terms most highly associated to each circuit over a range of list lengths.\n",
    "3. Select the list length for each circuit that maximizes word-structure classification performance. \n",
    "4. Select the number of circuits that maximizes circuit-function classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain activation coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document N=18155, Structure N=114\n"
     ]
    }
   ],
   "source": [
    "act_bin = load_coordinates()\n",
    "print(\"Document N={}, Structure N={}\".format(act_bin.shape[0], act_bin.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms for mental functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 190325\n",
    "dtm_bin = load_doc_term_matrix(version=version, binarize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = load_lexicon([\"cogneuro\"])\n",
    "lexicon = sorted(list(set(lexicon).intersection(dtm_bin.columns)))\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document N=18155, Term N=1683\n"
     ]
    }
   ],
   "source": [
    "dtm_bin = dtm_bin[lexicon]\n",
    "print(\"Document N={}, Term N={}\".format(dtm_bin.shape[0], dtm_bin.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training N=12708, Validation N=3631\n"
     ]
    }
   ],
   "source": [
    "train, val = [[int(pmid.strip()) for pmid in open(\"../data/splits/{}.txt\".format(split))] for split in [\"train\", \"validation\"]]\n",
    "print(\"Training N={}, Validation N={}\".format(len(train), len(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link structures to functions\n",
    "\n",
    "Links are computed as PMI-weighted co-occurrences across the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure N=114, Term N=1634\n"
     ]
    }
   ],
   "source": [
    "stm = np.dot(act_bin.loc[train].transpose(), dtm_bin.loc[train])\n",
    "stm = pd.DataFrame(stm, columns=dtm_bin.columns, index=act_bin.columns)\n",
    "stm = pmi(stm, positive=False)\n",
    "stm = stm.dropna(axis=1, how=\"all\") # Drop terms with no co-occurrences\n",
    "print(\"Structure N={}, Term N={}\".format(stm.shape[0], stm.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms most strongly linked to the left amygdala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "olfactory_stimulus_transduction      4.273455\n",
       "auditory_system_function             3.174843\n",
       "letter_naming_task                   2.887161\n",
       "eye_puff                             2.481696\n",
       "face_identification_task             2.427629\n",
       "waisinformation                      2.039863\n",
       "emotion_expression_identification    2.039863\n",
       "pavlovian_conditioning_task          2.022164\n",
       "social_norm_processing_task          1.970870\n",
       "offensive_aggression                 1.938081\n",
       "Name: left_amygdala, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stm.loc[\"left_amygdala\"].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structures most strongly linked to *face_identification_task*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "right_parahippocampal_gyrus_anterior_division    2.527504\n",
       "left_frontal_medial_cortex                       2.454311\n",
       "right_amygdala                                   2.451786\n",
       "left_parahippocampal_gyrus_anterior_division     2.435533\n",
       "right_frontal_medial_cortex                      2.429094\n",
       "left_amygdala                                    2.427629\n",
       "right_hippocampus                                2.116216\n",
       "left_temporal_pole                               2.013200\n",
       "right_temporal_pole                              1.988026\n",
       "left_cingulate_gyrus_anterior_division           1.569213\n",
       "Name: face_identification_task, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stm[\"face_identification_task\"].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the ontology\n",
    "\n",
    "## 1. Cluster brain structures by functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import pointbiserialr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_counts = range(2, 41) # Range over which ROC-AUC becomes asymptotic\n",
    "list_lens = range(5, 26) # Same range as RDoC and the DSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in circuit_counts:\n",
    "    circuit_file = \"circuits/circuits_k{:02d}.csv\".format(k)\n",
    "    if not os.path.isfile(circuit_file):\n",
    "        kmeans = KMeans(n_clusters=k, max_iter=1000, random_state=42)\n",
    "        kmeans.fit(stm)\n",
    "        clust = pd.DataFrame({\"STRUCTURE\": act_bin.columns, \n",
    "                              \"CLUSTER\": [l+1 for l in list(kmeans.labels_)]})\n",
    "        clust = clust.sort_values([\"CLUSTER\", \"STRUCTURE\"])\n",
    "        clust.to_csv(circuit_file, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify associated terms for mental functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ehbeam/anaconda/envs/ontol/lib/python3.6/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n"
     ]
    }
   ],
   "source": [
    "for k in circuit_counts:\n",
    "    circuit_file = \"circuits/circuits_k{:02d}.csv\".format(k)\n",
    "    clust = pd.read_csv(circuit_file, index_col=None)\n",
    "    outfile = \"lists/lists_k{:02d}.csv\".format(k)\n",
    "    if not os.path.isfile(outfile):\n",
    "        lists = pd.DataFrame()\n",
    "        for i in range(k):\n",
    "            structures = list(clust.loc[clust[\"CLUSTER\"] == i+1, \"STRUCTURE\"])\n",
    "            centroid = np.mean(act_bin.loc[train, structures], axis=1)\n",
    "            R = pd.Series([pointbiserialr(dtm_bin.loc[train, word], centroid)[0] \n",
    "                           for word in dtm_bin.columns], index=dtm_bin.columns)\n",
    "            R = R[R > 0].sort_values(ascending=False)[:max(list_lens)]\n",
    "            R = pd.DataFrame({\"CLUSTER\": [i+1 for l in range(max(list_lens))], \n",
    "                              \"TOKEN\": R.index, \"R\": R.values})\n",
    "            lists = lists.append(R)\n",
    "        lists.to_csv(outfile, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select number of words per domain\n",
    "\n",
    "Word list lengths were selected by mean ROC-AUC of forward and reverse classifiers for each circuit. The number of circuits will be selected across values of k using the lists of optimized length, this time training classifiers that use all the circuits at that k. All classifiers were optimized over a grid search for regularization strength, penalty, and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"forward\", \"reverse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = {}\n",
    "for d in directions:\n",
    "    fits[d] = {}\n",
    "    for k in circuit_counts:\n",
    "        fits[d][k] = pickle.load(open(\"sherlock/fits/{}_k{:02d}_oplen.p\".format(d, k), \"rb\"), encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select optimal number of domains\n",
    "\n",
    "### Circuit-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'lists/lists_k02_oplen.csv' does not exist: b'lists/lists_k02_oplen.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8073680859e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcircuit_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdomains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcircuits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ontology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfunction_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtm_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstructure_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Stanford/Research/Projects/Thesis/program/neuro-knowledge-engine/ontology/utilities.py\u001b[0m in \u001b[0;36mload_ontology\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_ontology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlist_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lists/lists_k{:02d}_oplen.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mlists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mcircuit_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"circuits/circuits_k{:02d}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mcircuits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuit_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ontol/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ontol/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ontol/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ontol/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ontol/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'lists/lists_k02_oplen.csv' does not exist: b'lists/lists_k02_oplen.csv'"
     ]
    }
   ],
   "source": [
    "features = {k: {} for k in circuit_counts}\n",
    "for k in circuit_counts:\n",
    "    domains = range(1, k+1)\n",
    "    lists, circuits = load_ontology(k)\n",
    "    function_features = pd.DataFrame(index=dtm_bin.index, columns=domains)\n",
    "    structure_features = pd.DataFrame(index=act_bin.index, columns=domains)\n",
    "    for i in domains:\n",
    "        functions = lists.loc[lists[\"CLUSTER\"] == i, \"TOKEN\"]\n",
    "        function_features[i] = dtm_bin[functions].sum(axis=1)\n",
    "        structures = circuits.loc[circuits[\"CLUSTER\"] == i, \"STRUCTURE\"]\n",
    "        structure_features[i] = act_bin[structures].sum(axis=1)\n",
    "    function_features = pd.DataFrame(doc_mean_thres(function_features), \n",
    "                                     index=dtm_bin.index, columns=domains)\n",
    "    structure_features = pd.DataFrame(binarize(structure_features), \n",
    "                                     index=act_bin.index, columns=domains)\n",
    "    features[k][\"function\"] = function_features\n",
    "    features[k][\"structure\"] = structure_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the receiver operating characteristic curve (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc_scores = compute_eval_scores(roc_auc_score, directions, features, fits, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc_boot = compute_eval_boot(roc_auc_score, directions, features, fits, val, n_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc_null = compute_eval_null(roc_auc_score, directions, features, fits, val, n_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions += [\"mean\"]\n",
    "shapes = [\">\", \"<\", \"D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction, shape in zip(directions, shapes):\n",
    "    plot_scores(rocauc_scores, direction, rocauc_boot, rocauc_null, \n",
    "                \"rocauc\", shape=shape, ylim=[0.485,0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name the domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_k = 7\n",
    "lists, circuits = load_ontology(op_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2name = {}\n",
    "for k in range(op_k):\n",
    "    degrees = term_degree_centrality(k+1, lists, circuits, dtm_bin, train)\n",
    "    name = nounify(degrees.index[0], dtm_bin).upper()\n",
    "    k2name[k+1] = name\n",
    "    print(\"{}: {}\".format(k+1, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export ontology with domain names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [6, 3, 7, 5, 2, 1, 4]\n",
    "names = [k2name[k] for k in order]\n",
    "k2order = {k: order.index(k)+1 for k in range(1,op_k+1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function term lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"ORDER\", \"CLUSTER\", \"DOMAIN\", \"TOKEN\", \"R\", \"ROC_AUC\"]\n",
    "lists[\"ORDER\"] = [k2order[k] for k in lists[\"CLUSTER\"]]\n",
    "lists[\"DOMAIN\"] = [k2name[k] for k in lists[\"CLUSTER\"]]\n",
    "lists = lists.sort_values([\"ORDER\", \"R\"], ascending=[True, False])\n",
    "lists = lists[columns]\n",
    "lists.to_csv(\"lists/lists_data-driven.csv\", index=None)\n",
    "lists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"ORDER\", \"CLUSTER\", \"DOMAIN\", \"STRUCTURE\"]\n",
    "circuits[\"ORDER\"] = [k2order[k] for k in circuits[\"CLUSTER\"]]\n",
    "circuits[\"DOMAIN\"] = [k2name[k] for k in circuits[\"CLUSTER\"]]\n",
    "circuits = circuits.sort_values([\"ORDER\", \"STRUCTURE\"])\n",
    "circuits = circuits[columns]\n",
    "circuits.to_csv(\"circuits/clusters_data-driven.csv\", index=None)\n",
    "circuits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_mat = pd.DataFrame(0.0, index=act_bin.columns, columns=names)\n",
    "for name in names:\n",
    "    structures = circuits.loc[circuits[\"DOMAIN\"] == name, \"STRUCTURE\"]\n",
    "    for structure in structures:\n",
    "        circuit_mat.loc[structure, name] = 1.0\n",
    "circuit_mat.to_csv(\"circuits/circuits_data-driven.csv\")\n",
    "circuit_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the term lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordclouds(\"data-driven\", names, lists, dtm_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = load_atlas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purples = make_cmap([(1,1,1), (0.365,0,0.878)])\n",
    "magentas = make_cmap([(1,1,1), (0.620,0,0.686)])\n",
    "yellows = make_cmap([(1,1,1), (0.937,0.749,0)])\n",
    "cmaps = [\"Blues\", magentas, yellows, \"Greens\", \"Reds\", purples, \"Oranges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_plane(circuit_mat, atlas, \"figures/circuits/data-driven\", \n",
    "          suffix=\"_z\", cmaps=cmaps, plane=\"z\", cbar=False, vmin=0.0, vmax=2.0,\n",
    "          verbose=False, print_fig=True, annotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plane in [\"x\", \"y\"]:\n",
    "    map_plane(circuit_mat, atlas, \"figures/circuits/data-driven\", \n",
    "          suffix=\"_\"+plane, cmaps=cmaps, plane=plane, cbar=False, vmin=0.0, vmax=2.0,\n",
    "          verbose=False, print_fig=False, annotate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Ontol)",
   "language": "python",
   "name": "ontol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
